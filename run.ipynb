{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd141aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/t-chuhanwu/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/t-chuhanwu/.cache/huggingface/datasets/glue/mnli_mismatched/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/t-chuhanwu/.cache/huggingface/datasets/glue/mnli_mismatched/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from parameters import *\n",
    "from prompt import *\n",
    "\n",
    "dataset_train = load_dataset('glue', DATASET, split='train')\n",
    "dataset_val = load_dataset('glue', DATASET, split='validation')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    if DATASET=='mnli':\n",
    "        return tokenizer(examples['premise'], examples['hypothesis'],  truncation=True, padding='max_length',max_length=128)\n",
    "    if DATASET=='qnli':\n",
    "        return tokenizer(examples['question'], examples['sentence'],  truncation=True, padding='max_length',max_length=128)\n",
    "    if DATASET=='qqp': \n",
    "        return tokenizer(examples['question1'], examples['question2'], truncation=True, padding='max_length',max_length=128)\n",
    "    if DATASET=='sst2': \n",
    "        return tokenizer(examples['sentence'],  truncation=True, padding='max_length',max_length=128)\n",
    "\n",
    "dataset_train = dataset_train.map(encode, batched=True)\n",
    "dataset_val = dataset_val.map(encode, batched=True)\n",
    "\n",
    "dataset_train = dataset_train.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "dataset_val = dataset_val.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "\n",
    "dataset_train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=CLIENT_NUM)\n",
    "dataset_val.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=CLIENT_NUM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a185ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoConfig\n",
    "\n",
    "\n",
    "config=AutoConfig.from_pretrained(MODEL)\n",
    "model = BertPrefixForSequenceClassification.from_pretrained(MODEL) #BertPromptForSequenceClassification/BertForSequenceClassification\n",
    "\n",
    "from tqdm import tqdm \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.train().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LR)\n",
    "\n",
    "for name,para in model.named_parameters():\n",
    "    if 'prefix_encoder' not in name and 'classifier' not in name  and 'pooler' not in name :\n",
    "        para.requires_grad=False\n",
    "    else:\n",
    "        para.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "import numpy as np\n",
    "norm_l2=NORMCLIP\n",
    "eps=10.\n",
    "delta=1e-3  \n",
    "all_loss=0.\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(tqdm(dataloader_train)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        all_loss+=loss.data\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), NORMCLIP)\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                # add equivalent noise after aggregation\n",
    "                p.grad += torch.FloatTensor(np.random.normal(0, 2*NORMCLIP*np.sqrt(2*np.log(1.25/DELTA))/EPS/np.sqrt(CLIENT_NUM),size=p.grad.size())).cuda()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"loss: {all_loss/(i+1)}\")\n",
    "    model.eval()  \n",
    "    all_pred=[]\n",
    "    all_label=[]\n",
    "    for i, batch in enumerate(tqdm(dataloader_val)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        all_pred += np.argmax(outputs[1].detach().cpu().numpy(),axis=-1).tolist()\n",
    "        all_label+=batch['labels'].detach().cpu().numpy().tolist() \n",
    "    print(accuracy_score(all_label,all_pred))\n",
    "    model.train()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
